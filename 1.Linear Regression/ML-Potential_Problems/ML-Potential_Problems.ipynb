{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Study Note: Linear Regression Part II - Potential Problems\"\n",
    "date: '2019–06–07 00:00:13'\n",
    "categories: Machine Learning\n",
    "mathjax: true\n",
    "abbrlink: 4df00c7b\n",
    "tags: \n",
    "- Linear Regression\n",
    "- Regression\n",
    "comments: true\n",
    "---\n",
    "\n",
    "\n",
    "# Qualitative Predictors\n",
    "\n",
    "## Predictors with Only Two Levels\n",
    "Suppose that we wish to investigate differences in credit card balance between\n",
    "males and females, ignoring the other variables for the moment. If a\n",
    "qualitative predictor (also known as a **factor**) only has two **levels**, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or **dummy variable** that takes on two possible numerical values.\n",
    "\n",
    "<img src=\"./8.png\" width=\"300\" />\n",
    "and use this variable as a predictor in the regression equation. This results\n",
    "in the model\n",
    "\n",
    "<img src=\"./9.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "<!--more-->\n",
    "\n",
    "\n",
    "Now β0 can be interpreted as the average credit card balance among males,\n",
    "β0 + β1 as the average credit card balance among females \n",
    "\n",
    "## Qualitative Predictors with More than Two Levels\n",
    "\n",
    "When a qualitative predictor has more than two levels, we can create\n",
    "additional dummy variables. For example, for the ethnicity variable we\n",
    "create two dummy variables. The first could be\n",
    "\n",
    "<img src=\"./10.png\" width=\"300\" />\n",
    "and the second could be\n",
    "<img src=\"./11.png\" width=\"300\" />\n",
    "\n",
    "Then both of these variables can be used in the regression equation, in\n",
    "order to obtain the model\n",
    "\n",
    "<img src=\"./12.png\" width=\"600\" />\n",
    "\n",
    "**Baseline**\n",
    "\n",
    "- There will always be **one fewer** dummy variable\n",
    "than the number of levels. The level with no dummy variable—African\n",
    "American in this example—is known as the baseline.\n",
    "\n",
    "<img src=\"./13.png\" width=\"600\" />\n",
    "\n",
    "The p-values associated with the coefficient estimates for\n",
    "the two dummy variables are very large, suggesting no statistical evidence\n",
    "of a real difference in credit card balance between the ethnicities\n",
    "\n",
    "> The coefficients and their p-values do depend on the choice of dummy\n",
    "variable coding\n",
    "\n",
    "Rather than rely on the individual coefficients, we can use\n",
    "an **F-test** to test H0 : β1 = β2 = 0; this does not depend on the coding.\n",
    "\n",
    "This F-test has a p-value of 0.96, indicating that we cannot reject the null\n",
    "hypothesis that there is no relationship between balance and ethnicity. \n",
    "\n",
    "# Extensions of the Linear Model\n",
    "\n",
    "Two\n",
    "of the most important assumptions state that the relationship between the\n",
    "predictors and response are **additive** and **linear**. \n",
    "- **Additive**: the effect of changes in a predictor $X_j$ on the response $Y$ is\n",
    "independent of the values of the other predictors\n",
    "- **Linear**: the change in the response $Y$ due to a one-unit change in $X_j$ is\n",
    "constant, regardless of the value of $X_j$\n",
    "\n",
    "## Removing the Additive Assumption\n",
    "Consider the standard linear regression model with two variables,\n",
    "\\begin{align}\n",
    "Y = β_0 + β_1X_1 + β_2X_2 + \\epsilon\n",
    "\\end{align}\n",
    "\n",
    "One way of extending this model\n",
    "to allow for interaction effects is to include a third predictor, called an\n",
    "**interaction term**:\n",
    "\n",
    "\\begin{align}\n",
    "Y = β_0 + β_1X_1 + β_2X_2 +  β_3X_1X_2 + \\epsilon \n",
    "\\end{align}\n",
    "\n",
    "**How does inclusion of this interaction term relax the additive assumption?**\n",
    "\n",
    "The model above could be written as:\n",
    "\\begin{align}\n",
    "Y &= β_0 + (β_1+β_3X_2)X_1 + β_2X_2 + \\epsilon  \\\\\n",
    "&= β_0 + \\tilde{β}_1X_1 + β_2X_2 + \\epsilon\n",
    "\\end{align}\n",
    "\n",
    "Since $\\tilde{β}_1$ changes with $X_2$, the effect of $X_1$ on $Y$ is\n",
    "no longer constant: adjusting $X_2$ will change the impact of $X_1$ on $Y$.\n",
    "\n",
    "<img src=\"./14.png\" width=\"600\" />\n",
    "\n",
    "- Sometimes the case that an interaction term has a very small p-value, but\n",
    "the associated main effects (in this case, TV and radio) do not. \n",
    "- The **hierarchical\n",
    "principle** states that if we include an interaction in a model, we\n",
    "should also include the **main effects**, even if the p-values associated with \n",
    "their coefficients are not significant. (If the interaction between\n",
    "X1 and X2 seems important, we should include both X1 and\n",
    "X2 in the model even if their coefficient estimates have large p-values)\n",
    "\n",
    "**Concept of\n",
    "interactions applies on qualitative variables**\n",
    "<img src=\"./15.png\" width=\"600\" />\n",
    "\n",
    "Adding an interaction variable, model now becomes:\n",
    "<img src=\"./17.png\" width=\"600\" />\n",
    "<img src=\"./16.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Non-linear Relationships\n",
    "\n",
    "Extending the linear model\n",
    "to accommodate non-linear relationships is known as **polynomial regression**,\n",
    "since we have included **polynomial functions** of the predictors in the\n",
    "regression model \n",
    "\n",
    "# Potential Problems \n",
    "\n",
    "## Non-linearity of the Data\n",
    "**Assumption**: The linear regression model assumes that there is a straight-line relationship\n",
    "between the predictors and the response.\n",
    "\n",
    "**Residual plots**: graphical tool for identifying non-linearity\n",
    "- Given a simple linear regression model, we can plot the residuals,$e_i = y_i-\\hat{y_i}$ versus the predictor $x_i$, or $\\hat{y_i}$ when there are  multiple predictors\n",
    "\n",
    "<img src=\"./18.png\" width=\"600\" />\n",
    "\n",
    "- Ideally, the residual plot will show no discernible pattern.\n",
    "- If the residual plot indicates non-linear associations in the\n",
    "data, then a simple approach is to use **non-linear transformations** of the\n",
    "predictors, such as $\\log{X},\\sqrt{X}, X^2$, in the regression model. \n",
    "\n",
    "## Correlation of Error Terms \n",
    "\n",
    "**Assumption**: The error terms, $\\epsilon_1,\\epsilon_2,...,\\epsilon_n$, are uncorrelated.\n",
    "\n",
    "-  If the errors are uncorrelated, then the fact that \u0003i is positive provides little or no information about the sign of \u0003i+1.\n",
    "-  If the error terms are correlated, we may have an unwarranted sense of confidence in our model.\n",
    " - **estimated standard errors** will underestimate the true standard errors.\n",
    " -  **confidence and prediction intervals** will be narrower than they should be. For example, a 95% confidence interval may in reality have a much lower probability than 0.95 of containing the true value of the parameter. \n",
    " - **p-values** will be lower than they should be\n",
    " - Lead to erroneously conclude that a parameter is statistically significant. \n",
    "\n",
    "\n",
    "**Why might correlations among the error terms occur?**\n",
    "- Such correlations\n",
    "frequently occur in the context of **time series** data\n",
    "- In many cases, observations that are obtained at adjacent time points will\n",
    "have **positively correlated errors**. \n",
    "- Plot the residuals from our model as a function of time to identify this correlation. \n",
    " - **Tracking**: If the error terms are positively correlated, then\n",
    "we may see **tracking** in the residuals—that is, adjacent residuals may have similar values.\n",
    "<img src=\"./19.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "## Non-constant Variance of Error Terms\n",
    "\n",
    "**Assumption**: the error terms have a constant variance, $Var(\\epsilon_i) = σ^2$.\n",
    "- The standard errors,\n",
    "confidence intervals, and hypothesis tests associated with the linear model\n",
    "rely upon this assumption.\n",
    "\n",
    "The variances of the error terms are non-constant. \n",
    "- For instance, the variances of the error terms may increase with the value of the response. \n",
    "- One can identify non-constant variances in\n",
    "the errors, or **heteroscedasticity**异方差性,from the presence of a funnel shape in residual plot.\n",
    "- **Solution**: transform the response Y using a concave function such as $\\log{Y}$ or $\\sqrt{Y}$ . Such\n",
    "a transformation results in a greater amount of shrinkage of the larger responses,\n",
    "leading to a reduction in **heteroscedasticity**.\n",
    "\n",
    "<img src=\"./20.png\" width=\"600\" />\n",
    "\n",
    "## Outliers \n",
    "\n",
    "**Outlier**: is a point for which $y_i$ is far from the value predicted by the model. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.\n",
    "\n",
    "**Problems of Outlier**:\n",
    "- Effect on the least squares fit,\n",
    "- Effect on interpretation of the fit\n",
    " - For instance, in this example, the RSE is 1.09 when the\n",
    "outlier is included in the regression, but it is only 0.77 when the outlier\n",
    "is removed.\n",
    "\n",
    "**Residual Plots** can be used to identify outliers\n",
    "\n",
    "<img src=\"./21.png\" width=\"600\" />\n",
    "\n",
    "- Difficult to decide how large a residual\n",
    "needs to be\n",
    "\n",
    "**Studentized residuals**: computed by dividing each residual $e_i$ by its estimated standard error.\n",
    "- Observations whose studentized residuals are greater than 3 in abso- residual\n",
    "lute value are possible outliers.\n",
    "\n",
    "## High Leverage Points \n",
    "\n",
    "**High Leverage**:Observations with high leverage have an unusual value for xi\n",
    "- removing the high leverage observation has a much more substantial impact on the least squares line\n",
    "than removing the outlier.\n",
    "<img src=\"./23.png\" width=\"600\" />\n",
    "\n",
    "**Leverage Statistic**: quantify an observation’s leverage\n",
    "\n",
    "For a simple linear regression\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h_i=\\frac{1}{n}+\\frac{(x_i-\\bar{x})^2}{\\sum_{i^{'}=1}^n (x_{i^{'}}-\\bar{x})^2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "- $h_i$ increases with the distance of $x_i$ from $\\bar{x}$.\n",
    "- $h_i$ is always between 1/n and 1, and the **average leverage** for all the observations is\n",
    "always equal to $(p+1)/n$. \n",
    "- **High leverage**: a leverage statistic that greatly exceeds $(p+1)/n$, high leverage.\n",
    "\n",
    "<img src=\"./22.png\" width=\"600\" />\n",
    "\n",
    "The right-hand panel of Figure 3.13 provides a plot of the studentized\n",
    "residuals versus $h_i$ for the data in the left-hand panel of Figure 3.13. Observation\n",
    "41 stands out as having a very high leverage statistic as well as a\n",
    "high studentized residual. In other words, it is an outlier as well as a high\n",
    "leverage observation.\n",
    "\n",
    "## Collinearity \n",
    "\n",
    "Collinearity: situation in which two or more predictor variables are closely related to one another.\n",
    "\n",
    "**Problems of Collinearity**\n",
    "\n",
    "- Difficult to separate out the individual\n",
    "effects of collinear variables on the response\n",
    "- Uncertainty in the\n",
    "coefficient estimates.\n",
    "- Causes the standard error for $\\hat{β_j}$ to grow\n",
    " - Recall that the\n",
    "t-statistic for each predictor is calculated by dividing $\\hat{β_j}$ by its standard\n",
    "error. Consequently, collinearity results in a decline in the t-statistic. As a\n",
    "result, in the presence of collinearity, we may fail to reject H0 : βj = 0. This\n",
    "means that the **power** of the hypothesis test—the probability of correctly\n",
    "detecting a non-zero coefficient—is reduced by collinearity.\n",
    "\n",
    "<img src=\"./24.png\" width=\"600\" />\n",
    "\n",
    "**Detection of Collinearity**\n",
    "\n",
    "- **Correlation matrix**\n",
    "of the predictors.\n",
    " - An element of this matrix that is large in absolute value\n",
    "indicates a pair of highly correlated variables.\n",
    " - **Situation Multicollinearity**: it is possible for collinearity\n",
    "to exist between three or more variables even if no pair of variables\n",
    "has a particularly high correlation\n",
    "- **Variance Inflation Factor (VIF)**\n",
    " - The ratio of the variance of $\\hat{β_j}$ when fitting the full model divided by the\n",
    "variance of $\\hat{β_j}$ if fit on its own. The smallest possible value for VIF is 1,\n",
    "which indicates the complete absence of collinearity.\n",
    " - A VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity. \n",
    "\n",
    " The VIF for each variable,\n",
    "\n",
    "where $R^2_{X_j|X_{-j}}$ is the $R^2$ from a regression of $X_j$ onto all of the other\n",
    "predictors. If $R^2_{X_j|X_{-j}}$ is close to one, then collinearity is present, and so\n",
    "the VIF will be large.\n",
    "\n",
    "**Solution of Collinearity**\n",
    "\n",
    "- Drop one of the problematic variables from the regression.\n",
    "- Combine the collinear variables together into a single predicto\n",
    " - E.g.: take the average of standardized versions of limit and\n",
    "rating in order to create a new variable that measures credit worthiness\n",
    "\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "**Ref:**\n",
    "\n",
    "James, Gareth, et al. *An introduction to statistical learning*. Vol. 112. New York: springer, 2013.\n",
    "\n",
    "Hastie, Trevor, et al. \"The elements of statistical learning: data mining, inference and prediction.\" *The Mathematical Intelligencer* 27.2 (2005): 83-85"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
